"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[8137],{8453:(n,e,i)=>{i.d(e,{R:()=>l,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function l(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),r.createElement(a.Provider,{value:e},n.children)}},9897:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"chapter6","title":"Chapter 6: Learning for Robotics","description":"Apply machine learning and reinforcement learning to physical agents.","source":"@site/docs/chapter6.md","sourceDirName":".","slug":"/chapter6","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter6","draft":false,"unlisted":false,"editUrl":"https://github.com/SidraRaza/Physical-AI-Humanoid-Robotics/edit/main/docs/chapter6.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Chapter 6: Learning for Robotics","description":"Apply machine learning and reinforcement learning to physical agents.","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Motion Planning & Control","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter5"},"next":{"title":"Chapter 7: Real-World Deployment & Ethics","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter7"}}');var s=i(4848),a=i(8453);const l={title:"Chapter 6: Learning for Robotics",description:"Apply machine learning and reinforcement learning to physical agents.",sidebar_position:6},o="Learning for Robotics",t={},d=[{value:"Introduction to Robot Learning",id:"introduction-to-robot-learning",level:2},{value:"Why Learning?",id:"why-learning",level:3},{value:"Learning Paradigms",id:"learning-paradigms",level:3},{value:"Imitation Learning",id:"imitation-learning",level:2},{value:"Learning from Demonstrations",id:"learning-from-demonstrations",level:3},{value:"DAgger (Dataset Aggregation)",id:"dagger-dataset-aggregation",level:3},{value:"Inverse Reinforcement Learning (IRL)",id:"inverse-reinforcement-learning-irl",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:2},{value:"RL Fundamentals",id:"rl-fundamentals",level:3},{value:"Policy Gradient Methods",id:"policy-gradient-methods",level:3},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Soft Actor-Critic (SAC)",id:"soft-actor-critic-sac",level:3},{value:"Model-Based RL",id:"model-based-rl",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"The Reality Gap",id:"the-reality-gap",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Domain Adaptation",id:"domain-adaptation",level:3},{value:"Progressive Transfer",id:"progressive-transfer",level:3},{value:"Learning for Manipulation",id:"learning-for-manipulation",level:2},{value:"Grasping",id:"grasping",level:3},{value:"Dexterous Manipulation",id:"dexterous-manipulation",level:3},{value:"Learning from Play",id:"learning-from-play",level:3},{value:"Learning for Locomotion",id:"learning-for-locomotion",level:2},{value:"Policy Learning for Walking",id:"policy-learning-for-walking",level:3},{value:"Reward Shaping",id:"reward-shaping",level:3},{value:"Terrain Adaptation",id:"terrain-adaptation",level:3},{value:"Foundation Models for Robotics",id:"foundation-models-for-robotics",level:2},{value:"Vision-Language Models",id:"vision-language-models",level:3},{value:"Robot Foundation Models",id:"robot-foundation-models",level:3},{value:"Practical Considerations",id:"practical-considerations",level:2},{value:"Data Collection",id:"data-collection",level:3},{value:"Safety During Learning",id:"safety-during-learning",level:3},{value:"Evaluation",id:"evaluation",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"learning-for-robotics",children:"Learning for Robotics"})}),"\n",(0,s.jsx)(e.p,{children:"Machine learning has revolutionized robotics, enabling robots to acquire skills that are difficult or impossible to program explicitly. This chapter explores how learning algorithms are applied to physical AI systems, from perception to control."}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-robot-learning",children:"Introduction to Robot Learning"}),"\n",(0,s.jsx)(e.h3,{id:"why-learning",children:"Why Learning?"}),"\n",(0,s.jsx)(e.p,{children:"Traditional robotics relies on explicit programming, but many tasks are:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Too Complex"}),": Cannot enumerate all situations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environment-Dependent"}),": Require adaptation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Skill-Based"}),": Better learned than designed"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Data-Rich"}),": Can leverage experience"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"learning-paradigms",children:"Learning Paradigms"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Robot Learning Paradigms                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Supervised     \u2502   Reinforcement  \u2502    Self-Supervised    \u2502\n\u2502   Learning       \u2502   Learning       \u2502    Learning           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Human provides   \u2502 Robot explores,  \u2502 Robot generates own   \u2502\n\u2502 labeled examples \u2502 receives rewards \u2502 supervision signals   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 Classification \u2502 \u2022 Policy learning\u2502 \u2022 Contrastive learning\u2502\n\u2502 \u2022 Regression     \u2502 \u2022 Value learning \u2502 \u2022 Prediction tasks    \u2502\n\u2502 \u2022 Imitation      \u2502 \u2022 Model learning \u2502 \u2022 World models        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h2,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,s.jsx)(e.h3,{id:"learning-from-demonstrations",children:"Learning from Demonstrations"}),"\n",(0,s.jsx)(e.p,{children:"Robots learn by observing human demonstrations:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Behavioral Cloning (BC)"}),"\nDirect supervised learning on state-action pairs:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Collect demonstrations\ndemonstrations = []\nfor episode in human_demonstrations:\n    for (state, action) in episode:\n        demonstrations.append((state, action))\n\n# Train policy network\npolicy = NeuralNetwork()\noptimizer = Adam(policy.parameters())\n\nfor batch in DataLoader(demonstrations):\n    states, actions = batch\n    predicted_actions = policy(states)\n    loss = mse_loss(predicted_actions, actions)\n    loss.backward()\n    optimizer.step()\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Limitations of BC:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Distribution shift: Robot may encounter unseen states"}),"\n",(0,s.jsx)(e.li,{children:"Compounding errors: Small mistakes accumulate"}),"\n",(0,s.jsx)(e.li,{children:"No recovery behavior: Demonstrations show success only"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"dagger-dataset-aggregation",children:"DAgger (Dataset Aggregation)"}),"\n",(0,s.jsx)(e.p,{children:"Addresses distribution shift by iteratively collecting data:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Algorithm DAgger:\n1. Train initial policy \u03c0\u2081 on human demonstrations D\n2. For iteration n = 1 to N:\n   a. Execute policy \u03c0\u2099 to collect trajectories\n   b. Query human expert for correct actions\n   c. Add new (state, expert_action) to D\n   d. Train \u03c0\u2099\u208a\u2081 on aggregated dataset D\n"})}),"\n",(0,s.jsx)(e.h3,{id:"inverse-reinforcement-learning-irl",children:"Inverse Reinforcement Learning (IRL)"}),"\n",(0,s.jsx)(e.p,{children:"Learn the reward function from demonstrations:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Given: Expert demonstrations\nLearn: Reward function R(s, a)\nThen: Use R to train optimal policy\n\nKey insight: The expert's behavior implies their reward function\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Maximum Entropy IRL:"}),"\nAssumes expert is optimal with some randomness, finds reward that explains demonstrations while maximizing entropy."]}),"\n",(0,s.jsx)(e.h2,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,s.jsx)(e.h3,{id:"rl-fundamentals",children:"RL Fundamentals"}),"\n",(0,s.jsx)(e.p,{children:"The robot learns through trial and error:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    action a    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Agent  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 Environment \u2502\n\u2502 (Robot) \u2502                \u2502   (World)   \u2502\n\u2502         \u2502 \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  state s,      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             reward r\n\nGoal: Maximize cumulative reward\nV(s) = E[\u03a3 \u03b3\u1d57 r_t | s\u2080 = s]\n"})}),"\n",(0,s.jsx)(e.h3,{id:"policy-gradient-methods",children:"Policy Gradient Methods"}),"\n",(0,s.jsx)(e.p,{children:"Directly optimize the policy:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"REINFORCE Algorithm:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def reinforce_update(policy, trajectories):\n    policy_loss = 0\n\n    for trajectory in trajectories:\n        returns = compute_returns(trajectory.rewards, gamma)\n\n        for t, (state, action, ret) in enumerate(zip(\n            trajectory.states,\n            trajectory.actions,\n            returns\n        )):\n            log_prob = policy.log_prob(state, action)\n            policy_loss -= log_prob * ret\n\n    policy_loss /= len(trajectories)\n    policy_loss.backward()\n    optimizer.step()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,s.jsx)(e.p,{children:"State-of-the-art policy gradient method:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Key Ideas:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Clipped surrogate objective for stable updates"}),"\n",(0,s.jsx)(e.li,{children:"Multiple epochs per batch of experience"}),"\n",(0,s.jsx)(e.li,{children:"Trust region without complex constraints"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def ppo_loss(old_log_probs, new_log_probs, advantages, clip_range=0.2):\n    ratio = torch.exp(new_log_probs - old_log_probs)\n\n    # Clipped objective\n    clipped_ratio = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n\n    loss = -torch.min(\n        ratio * advantages,\n        clipped_ratio * advantages\n    ).mean()\n\n    return loss\n"})}),"\n",(0,s.jsx)(e.h3,{id:"soft-actor-critic-sac",children:"Soft Actor-Critic (SAC)"}),"\n",(0,s.jsx)(e.p,{children:"Off-policy algorithm with entropy regularization:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Objective:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"J(\u03c0) = \u03a3 E[r(s,a) + \u03b1 H(\u03c0(\xb7|s))]\n\nWhere H is entropy, encouraging exploration\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Benefits:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Sample efficient (off-policy)"}),"\n",(0,s.jsx)(e.li,{children:"Robust exploration"}),"\n",(0,s.jsx)(e.li,{children:"Stable training"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"model-based-rl",children:"Model-Based RL"}),"\n",(0,s.jsx)(e.p,{children:"Learn a model of the environment:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               Model-Based RL Pipeline                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  Real Experience \u2500\u2500\u25ba Learn Dynamics Model \u2500\u2500\u25ba               \u2502\n\u2502                     s_{t+1} = f(s_t, a_t)                   \u2502\n\u2502                                                             \u2502\n\u2502  Use Model for:                                             \u2502\n\u2502  \u2022 Planning (MPC)                                           \u2502\n\u2502  \u2022 Generating synthetic experience                          \u2502\n\u2502  \u2022 Policy optimization                                      \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Advantages:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Sample efficient"}),"\n",(0,s.jsx)(e.li,{children:"Better generalization"}),"\n",(0,s.jsx)(e.li,{children:"Interpretable"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Challenges:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Model errors compound"}),"\n",(0,s.jsx)(e.li,{children:"Complex dynamics hard to learn"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(e.h3,{id:"the-reality-gap",children:"The Reality Gap"}),"\n",(0,s.jsx)(e.p,{children:"Policies trained in simulation often fail in reality due to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Visual differences"}),": Rendering vs. real images"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Dynamics differences"}),": Simplified physics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor noise"}),": Idealized sensors in sim"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Unmodeled effects"}),": Friction, backlash, delays"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsx)(e.p,{children:"Randomize simulation parameters during training:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class RandomizedEnvironment:\n    def reset(self):\n        # Randomize physics\n        self.friction = np.random.uniform(0.5, 1.5)\n        self.mass_scale = np.random.uniform(0.8, 1.2)\n        self.motor_strength = np.random.uniform(0.9, 1.1)\n\n        # Randomize visuals\n        self.lighting = np.random.uniform(0.5, 1.5)\n        self.texture_id = np.random.randint(0, 100)\n        self.camera_noise = np.random.uniform(0, 0.1)\n\n        # Randomize timing\n        self.action_delay = np.random.randint(0, 3)\n\n        return self.get_observation()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"domain-adaptation",children:"Domain Adaptation"}),"\n",(0,s.jsx)(e.p,{children:"Adapt representations across domains:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Feature Alignment:"}),"\nLearn representations that are domain-invariant"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Adversarial Training:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Domain Adversarial Training                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  Sim Images \u2500\u2500\u2510                                             \u2502\n\u2502               \u251c\u2500\u2500\u25ba Encoder \u2500\u2500\u25ba Features \u2500\u2500\u25ba Task Head       \u2502\n\u2502  Real Images \u2500\u2518        \u2502                                    \u2502\n\u2502                        \u2514\u2500\u2500\u25ba Domain Classifier (adversarial) \u2502\n\u2502                                                             \u2502\n\u2502  Goal: Features that solve task but fool domain classifier \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h3,{id:"progressive-transfer",children:"Progressive Transfer"}),"\n",(0,s.jsx)(e.p,{children:"Gradually increase realism:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Train in simple simulation"}),"\n",(0,s.jsx)(e.li,{children:"Add visual complexity"}),"\n",(0,s.jsx)(e.li,{children:"Add dynamics complexity"}),"\n",(0,s.jsx)(e.li,{children:"Add noise and delays"}),"\n",(0,s.jsx)(e.li,{children:"Deploy to real robot"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"learning-for-manipulation",children:"Learning for Manipulation"}),"\n",(0,s.jsx)(e.h3,{id:"grasping",children:"Grasping"}),"\n",(0,s.jsx)(e.p,{children:"Learning robust grasping policies:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Approaches:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Grasp Quality Networks"}),": Predict grasp success from images"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"End-to-End Learning"}),": Direct image to grasp action"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Self-Supervised"}),": Robot tries grasps, learns from outcomes"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class GraspNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = ResNet18(pretrained=True)\n        self.grasp_head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 6)  # x, y, z, roll, pitch, yaw\n        )\n\n    def forward(self, image):\n        features = self.encoder(image)\n        grasp_pose = self.grasp_head(features)\n        return grasp_pose\n"})}),"\n",(0,s.jsx)(e.h3,{id:"dexterous-manipulation",children:"Dexterous Manipulation"}),"\n",(0,s.jsx)(e.p,{children:"Learning complex hand manipulation:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Challenges:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"High-dimensional action space"}),"\n",(0,s.jsx)(e.li,{children:"Contact-rich dynamics"}),"\n",(0,s.jsx)(e.li,{children:"Long-horizon tasks"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Approaches:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Curriculum learning: Start simple, increase complexity"}),"\n",(0,s.jsx)(e.li,{children:"Privileged learning: Use extra info in training"}),"\n",(0,s.jsx)(e.li,{children:"Teacher-student: Distill privileged policy"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"learning-from-play",children:"Learning from Play"}),"\n",(0,s.jsx)(e.p,{children:"Self-supervised manipulation learning:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Robot "plays" with objects'}),"\n",(0,s.jsx)(e.li,{children:"No specific task reward"}),"\n",(0,s.jsx)(e.li,{children:"Learns diverse skills"}),"\n",(0,s.jsx)(e.li,{children:"Later fine-tuned for specific tasks"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"learning-for-locomotion",children:"Learning for Locomotion"}),"\n",(0,s.jsx)(e.h3,{id:"policy-learning-for-walking",children:"Policy Learning for Walking"}),"\n",(0,s.jsx)(e.p,{children:"Training humanoid locomotion:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def locomotion_reward(state, action):\n    reward = 0\n\n    # Forward velocity reward\n    reward += 2.0 * state.forward_velocity\n\n    # Alive bonus\n    reward += 1.0\n\n    # Energy penalty\n    reward -= 0.01 * np.sum(action ** 2)\n\n    # Posture penalty\n    reward -= 0.1 * (state.torso_angle ** 2)\n\n    return reward\n"})}),"\n",(0,s.jsx)(e.h3,{id:"reward-shaping",children:"Reward Shaping"}),"\n",(0,s.jsx)(e.p,{children:"Guiding learning with intermediate rewards:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Potential-Based Shaping:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"R_shaped(s, a, s') = R(s, a, s') + \u03b3\u03a6(s') - \u03a6(s)\n\nWhere \u03a6 is a potential function (e.g., distance to goal)\nThis preserves optimal policy while accelerating learning.\n"})}),"\n",(0,s.jsx)(e.h3,{id:"terrain-adaptation",children:"Terrain Adaptation"}),"\n",(0,s.jsx)(e.p,{children:"Learning to handle varied terrain:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Explicit terrain sensing"}),": Height maps, classification"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Implicit adaptation"}),": Learn from proprioception"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory-based"}),": Remember terrain characteristics"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"foundation-models-for-robotics",children:"Foundation Models for Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,s.jsx)(e.p,{children:"Using pretrained models for robotics:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Applications:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Zero-shot object recognition"}),"\n",(0,s.jsx)(e.li,{children:"Natural language task specification"}),"\n",(0,s.jsx)(e.li,{children:"Scene understanding"}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def language_conditioned_policy(image, instruction):\n    # Encode image and language\n    image_features = clip.encode_image(image)\n    text_features = clip.encode_text(instruction)\n\n    # Combined representation\n    combined = torch.cat([image_features, text_features], dim=-1)\n\n    # Policy network\n    action = policy_network(combined)\n    return action\n"})}),"\n",(0,s.jsx)(e.h3,{id:"robot-foundation-models",children:"Robot Foundation Models"}),"\n",(0,s.jsx)(e.p,{children:"Large-scale models trained on robot data:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"RT-1, RT-2 (Google):"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Trained on diverse robot experience"}),"\n",(0,s.jsx)(e.li,{children:"Multi-task, multi-embodiment"}),"\n",(0,s.jsx)(e.li,{children:"Language-conditioned"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Gato (DeepMind):"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Single network for multiple tasks"}),"\n",(0,s.jsx)(e.li,{children:"Images, text, actions unified"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"practical-considerations",children:"Practical Considerations"}),"\n",(0,s.jsx)(e.h3,{id:"data-collection",children:"Data Collection"}),"\n",(0,s.jsx)(e.p,{children:"Efficient data collection for robot learning:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Teleoperation"}),": Human controls robot"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Kinesthetic teaching"}),": Physical guidance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Autonomous exploration"}),": Robot collects own data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Simulation"}),": Generate large datasets cheaply"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"safety-during-learning",children:"Safety During Learning"}),"\n",(0,s.jsx)(e.p,{children:"Ensuring safety while robot explores:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class SafeExplorer:\n    def __init__(self, policy, safety_critic):\n        self.policy = policy\n        self.safety_critic = safety_critic\n\n    def get_action(self, state):\n        proposed_action = self.policy(state)\n\n        # Check safety\n        if self.safety_critic(state, proposed_action) < threshold:\n            # Action is unsafe, use backup\n            return self.safe_backup_action(state)\n\n        return proposed_action\n"})}),"\n",(0,s.jsx)(e.h3,{id:"evaluation",children:"Evaluation"}),"\n",(0,s.jsx)(e.p,{children:"Measuring learning success:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Success rate"}),": Task completion percentage"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sample efficiency"}),": Experience needed to learn"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Generalization"}),": Performance on unseen scenarios"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robustness"}),": Performance under perturbations"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(e.p,{children:"Machine learning enables robots to acquire complex skills:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Imitation Learning"}),": Learning from demonstrations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reinforcement Learning"}),": Learning from trial and error"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sim-to-Real Transfer"}),": Training in simulation, deploying in reality"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Manipulation Learning"}),": Grasping and dexterous skills"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Locomotion Learning"}),": Walking and navigation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Foundation Models"}),": Large-scale pretrained models"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Key challenges:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Sample efficiency"}),"\n",(0,s.jsx)(e.li,{children:"Safety during learning"}),"\n",(0,s.jsx)(e.li,{children:"Generalization to new situations"}),"\n",(0,s.jsx)(e.li,{children:"Bridging the reality gap"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Compare behavioral cloning with DAgger."}),"\n",(0,s.jsx)(e.li,{children:"Explain the exploration-exploitation tradeoff in RL."}),"\n",(0,s.jsx)(e.li,{children:"What is domain randomization and why is it useful?"}),"\n",(0,s.jsx)(e.li,{children:"How do foundation models change robot learning?"}),"\n",(0,s.jsx)(e.li,{children:"What safety considerations are important during robot learning?"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Exercise 6.1: Learning a Simple Manipulation Skill"})}),"\n",(0,s.jsx)(e.p,{children:"Train a robot to reach a target position:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Set up a simulation environment (PyBullet or Isaac Gym)"}),"\n",(0,s.jsx)(e.li,{children:"Define observation space (joint positions, target position)"}),"\n",(0,s.jsx)(e.li,{children:"Define action space (joint velocities or torques)"}),"\n",(0,s.jsx)(e.li,{children:"Implement a reward function for reaching"}),"\n",(0,s.jsx)(e.li,{children:"Train using PPO"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate on held-out target positions"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Extensions:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Add obstacles and train collision avoidance"}),"\n",(0,s.jsx)(e.li,{children:"Add domain randomization"}),"\n",(0,s.jsx)(e.li,{children:"Transfer to a different robot morphology"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}}}]);