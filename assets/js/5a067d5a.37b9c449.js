"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[5456],{1449:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapter4","title":"Chapter 4: Perception Systems","description":"Understand vision, depth sensing, SLAM, and sensor fusion in robots.","source":"@site/docs/chapter4.md","sourceDirName":".","slug":"/chapter4","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter4","draft":false,"unlisted":false,"editUrl":"https://github.com/SidraRaza/Physical-AI-Humanoid-Robotics/edit/main/docs/chapter4.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Chapter 4: Perception Systems","description":"Understand vision, depth sensing, SLAM, and sensor fusion in robots.","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: ROS 2 Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter3"},"next":{"title":"Chapter 5: Motion Planning & Control","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter5"}}');var r=i(4848),l=i(8453);const t={title:"Chapter 4: Perception Systems",description:"Understand vision, depth sensing, SLAM, and sensor fusion in robots.",sidebar_position:4},o="Perception Systems",c={},d=[{value:"Introduction to Robot Perception",id:"introduction-to-robot-perception",level:2},{value:"The Perception Challenge",id:"the-perception-challenge",level:3},{value:"The Perception Pipeline",id:"the-perception-pipeline",level:3},{value:"Vision Sensors",id:"vision-sensors",level:2},{value:"RGB Cameras",id:"rgb-cameras",level:3},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"Event Cameras",id:"event-cameras",level:3},{value:"LIDAR Systems",id:"lidar-systems",level:2},{value:"Operating Principles",id:"operating-principles",level:3},{value:"LIDAR Types",id:"lidar-types",level:3},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Deep Learning Approaches",id:"deep-learning-approaches",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"Instance Segmentation",id:"instance-segmentation",level:3},{value:"SLAM: Simultaneous Localization and Mapping",id:"slam-simultaneous-localization-and-mapping",level:2},{value:"The SLAM Problem",id:"the-slam-problem",level:3},{value:"Visual SLAM",id:"visual-slam",level:3},{value:"LIDAR SLAM",id:"lidar-slam",level:3},{value:"Semantic SLAM",id:"semantic-slam",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Why Fuse Sensors?",id:"why-fuse-sensors",level:3},{value:"Fusion Approaches",id:"fusion-approaches",level:3},{value:"Kalman Filter",id:"kalman-filter",level:3},{value:"Extended Kalman Filter (EKF)",id:"extended-kalman-filter-ekf",level:3},{value:"Particle Filters",id:"particle-filters",level:3},{value:"Pose Estimation",id:"pose-estimation",level:2},{value:"6-DoF Object Pose",id:"6-dof-object-pose",level:3},{value:"Human Pose Estimation",id:"human-pose-estimation",level:3},{value:"State Estimation",id:"state-estimation",level:2},{value:"Robot State Estimation",id:"robot-state-estimation",level:3},{value:"Contact State Estimation",id:"contact-state-estimation",level:3},{value:"Perception for Manipulation",id:"perception-for-manipulation",level:2},{value:"Grasp Detection",id:"grasp-detection",level:3},{value:"Scene Understanding",id:"scene-understanding",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"perception-systems",children:"Perception Systems"})}),"\n",(0,r.jsx)(n.p,{children:'Perception is the foundation of Physical AI, enabling robots to understand and interact with their environment. This chapter explores the sensors, algorithms, and systems that give robots the ability to "see" and comprehend the world around them.'}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-robot-perception",children:"Introduction to Robot Perception"}),"\n",(0,r.jsx)(n.h3,{id:"the-perception-challenge",children:"The Perception Challenge"}),"\n",(0,r.jsx)(n.p,{children:"Robots face unique perception challenges compared to humans:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Limitations"}),": No sensor perfectly captures reality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-Time Requirements"}),": Processing must keep pace with robot movement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Uncertainty"}),": All measurements contain noise and errors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Environments"}),": The world constantly changes"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"the-perception-pipeline",children:"The Perception Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"A typical perception system follows this flow:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Perception Pipeline                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  Sensors \u2500\u2500\u25ba Preprocessing \u2500\u2500\u25ba Feature Extraction \u2500\u2500\u25ba       \u2502\n\u2502                                                              \u2502\n\u2502  \u2500\u2500\u25ba Object Recognition \u2500\u2500\u25ba Scene Understanding \u2500\u2500\u25ba         \u2502\n\u2502                                                              \u2502\n\u2502  \u2500\u2500\u25ba World Model Update                                     \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"vision-sensors",children:"Vision Sensors"}),"\n",(0,r.jsx)(n.h3,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,r.jsx)(n.p,{children:"Standard cameras capture color images:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": Pixel dimensions (e.g., 1920\xd71080)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Frame Rate"}),": Images per second (FPS)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Field of View (FOV)"}),": Angular coverage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic Range"}),": Light sensitivity range"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Camera Models:"}),"\nThe pinhole camera model describes image formation:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"u = fx * (X/Z) + cx\nv = fy * (Y/Z) + cy\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"(X, Y, Z): 3D point coordinates"}),"\n",(0,r.jsx)(n.li,{children:"(u, v): Pixel coordinates"}),"\n",(0,r.jsx)(n.li,{children:"(fx, fy): Focal lengths"}),"\n",(0,r.jsx)(n.li,{children:"(cx, cy): Principal point"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,r.jsx)(n.p,{children:"Depth cameras provide per-pixel distance measurements:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Structured Light"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Projects known patterns onto the scene"}),"\n",(0,r.jsx)(n.li,{children:"Measures pattern distortion to compute depth"}),"\n",(0,r.jsx)(n.li,{children:"Example: Intel RealSense D400 series"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Time of Flight (ToF)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measures light round-trip time"}),"\n",(0,r.jsx)(n.li,{children:"Direct depth measurement"}),"\n",(0,r.jsx)(n.li,{children:"Example: Microsoft Azure Kinect"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Stereo Vision"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Uses two cameras to triangulate depth"}),"\n",(0,r.jsx)(n.li,{children:"Mimics human binocular vision"}),"\n",(0,r.jsx)(n.li,{children:"Requires texture for matching"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"event-cameras",children:"Event Cameras"}),"\n",(0,r.jsx)(n.p,{children:"Neuromorphic sensors inspired by biological vision:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Asynchronous Output"}),": Events triggered by brightness change"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"High Dynamic Range"}),": >120 dB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low Latency"}),": Microsecond response"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low Power"}),": Only responds to changes"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Applications:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High-speed tracking"}),"\n",(0,r.jsx)(n.li,{children:"HDR scene capture"}),"\n",(0,r.jsx)(n.li,{children:"Low-latency control"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"lidar-systems",children:"LIDAR Systems"}),"\n",(0,r.jsx)(n.h3,{id:"operating-principles",children:"Operating Principles"}),"\n",(0,r.jsx)(n.p,{children:"LIDAR (Light Detection and Ranging) uses laser pulses:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               LIDAR Operation               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                             \u2502\n\u2502  Laser Pulse \u2500\u2500\u25ba Object \u2500\u2500\u25ba Return Signal   \u2502\n\u2502       \u2502                          \u2502          \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500 Time of Flight \u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502                                             \u2502\n\u2502  Distance = (Speed of Light \xd7 Time) / 2    \u2502\n\u2502                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"lidar-types",children:"LIDAR Types"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mechanical Scanning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Rotating mirror/prism"}),"\n",(0,r.jsx)(n.li,{children:"Wide FOV coverage"}),"\n",(0,r.jsx)(n.li,{children:"Example: Velodyne, Ouster"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solid-State"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No moving parts"}),"\n",(0,r.jsx)(n.li,{children:"More reliable, lower cost"}),"\n",(0,r.jsx)(n.li,{children:"Limited FOV"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Flash LIDAR"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Illuminates entire scene at once"}),"\n",(0,r.jsx)(n.li,{children:"Very fast capture"}),"\n",(0,r.jsx)(n.li,{children:"Limited range"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,r.jsx)(n.p,{children:"LIDAR produces 3D point clouds:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\n# Point cloud structure\npoint_cloud = np.array([\n    [x1, y1, z1, intensity1],\n    [x2, y2, z2, intensity2],\n    # ... millions of points\n])\n\n# Ground plane segmentation using RANSAC\ndef segment_ground(points, threshold=0.1, iterations=100):\n    best_plane = None\n    best_inliers = []\n\n    for _ in range(iterations):\n        # Sample 3 random points\n        sample = points[np.random.choice(len(points), 3)]\n        # Fit plane\n        plane = fit_plane(sample)\n        # Count inliers\n        distances = point_to_plane_distance(points, plane)\n        inliers = np.where(distances < threshold)[0]\n\n        if len(inliers) > len(best_inliers):\n            best_inliers = inliers\n            best_plane = plane\n\n    return best_plane, best_inliers\n"})}),"\n",(0,r.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,r.jsx)(n.h3,{id:"deep-learning-approaches",children:"Deep Learning Approaches"}),"\n",(0,r.jsx)(n.p,{children:"Modern perception relies heavily on neural networks:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2D Object Detection"})}),"\n",(0,r.jsx)(n.p,{children:"Popular architectures:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLO"})," (You Only Look Once): Real-time detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Faster R-CNN"}),": High accuracy, two-stage"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SSD"})," (Single Shot Detector): Balance of speed/accuracy"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Example using a pre-trained detector\nimport torch\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()\n\n# Inference\nwith torch.no_grad():\n    predictions = model(image_tensor)\n\n# predictions contains:\n# - boxes: Bounding box coordinates\n# - labels: Class predictions\n# - scores: Confidence scores\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3D Object Detection"})}),"\n",(0,r.jsx)(n.p,{children:"Working with point clouds:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PointNet/PointNet++"}),": Direct point cloud processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VoxelNet"}),": Voxel-based representation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PointPillars"}),": Pillar-based encoding"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,r.jsx)(n.p,{children:"Labeling every pixel/point with a class:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Input Image                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 [Car] [Road] [Person] [Sky]  \u2502      \u2502\n\u2502  \u2502  \u2588\u2588\u2588\u2588   \u2591\u2591\u2591\u2591   \u2593\u2593\u2593\u2593   ____  \u2502      \u2502\n\u2502  \u2502  \u2588\u2588\u2588\u2588   \u2591\u2591\u2591\u2591   \u2593\u2593\u2593\u2593   ____  \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502           \u2193 CNN                        \u2502\n\u2502      Segmentation Mask                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 Each pixel labeled with class \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,r.jsx)(n.p,{children:"Combining detection with segmentation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identifies individual object instances"}),"\n",(0,r.jsx)(n.li,{children:"Provides precise object boundaries"}),"\n",(0,r.jsx)(n.li,{children:"Example: Mask R-CNN"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"slam-simultaneous-localization-and-mapping",children:"SLAM: Simultaneous Localization and Mapping"}),"\n",(0,r.jsx)(n.h3,{id:"the-slam-problem",children:"The SLAM Problem"}),"\n",(0,r.jsx)(n.p,{children:"SLAM solves two interrelated problems:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localization"}),": Where am I in the map?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mapping"}),": What does the environment look like?"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The chicken-and-egg nature: need a map to localize, need position to build map."}),"\n",(0,r.jsx)(n.h3,{id:"visual-slam",children:"Visual SLAM"}),"\n",(0,r.jsx)(n.p,{children:"Using cameras for SLAM:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Feature-Based SLAM"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Feature-Based Visual SLAM             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                 \u2502\n\u2502  Image \u2500\u2500\u25ba Feature Detection \u2500\u2500\u25ba Feature        \u2502\n\u2502            (ORB, SIFT, etc.)     Matching       \u2502\n\u2502                                     \u2502           \u2502\n\u2502                                     \u25bc           \u2502\n\u2502  Map \u25c4\u2500\u2500 Bundle Adjustment \u25c4\u2500\u2500 Motion          \u2502\n\u2502  Update      & Loop Closure     Estimation     \u2502\n\u2502                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.p,{children:"Key algorithms:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3"}),": State-of-the-art visual SLAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LSD-SLAM"}),": Direct (featureless) approach"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DSO"}),": Direct Sparse Odometry"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Direct SLAM"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Uses pixel intensities directly"}),"\n",(0,r.jsx)(n.li,{children:"No explicit feature detection"}),"\n",(0,r.jsx)(n.li,{children:"Better in low-texture environments"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lidar-slam",children:"LIDAR SLAM"}),"\n",(0,r.jsx)(n.p,{children:"Using LIDAR for mapping:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Scan Matching Algorithms"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ICP"})," (Iterative Closest Point): Aligns consecutive scans"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NDT"})," (Normal Distributions Transform): Probabilistic matching"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LOAM"}),": LIDAR Odometry and Mapping"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Graph-Based SLAM"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Pose Graph:\n\n   [P0]\u2500\u2500\u2500[P1]\u2500\u2500\u2500[P2]\u2500\u2500\u2500[P3]\n     \\                    /\n      \\\u2500\u2500\u2500\u2500 Loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500/\n           Closure\n\nOptimize poses to minimize:\n- Odometry errors between consecutive poses\n- Loop closure constraint errors\n"})}),"\n",(0,r.jsx)(n.h3,{id:"semantic-slam",children:"Semantic SLAM"}),"\n",(0,r.jsx)(n.p,{children:"Adding semantic understanding:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Label map elements (walls, doors, furniture)"}),"\n",(0,r.jsx)(n.li,{children:'Enable semantic queries ("where is the table?")'}),"\n",(0,r.jsx)(n.li,{children:"Improve loop closure with semantic features"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"why-fuse-sensors",children:"Why Fuse Sensors?"}),"\n",(0,r.jsx)(n.p,{children:"Different sensors have complementary strengths:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensor"}),(0,r.jsx)(n.th,{children:"Strengths"}),(0,r.jsx)(n.th,{children:"Weaknesses"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Camera"}),(0,r.jsx)(n.td,{children:"Rich appearance, cheap"}),(0,r.jsx)(n.td,{children:"No depth, lighting dependent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"LIDAR"}),(0,r.jsx)(n.td,{children:"Accurate depth, lighting invariant"}),(0,r.jsx)(n.td,{children:"Expensive, sparse data"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"IMU"}),(0,r.jsx)(n.td,{children:"High frequency, orientation"}),(0,r.jsx)(n.td,{children:"Drift over time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPS"}),(0,r.jsx)(n.td,{children:"Global position"}),(0,r.jsx)(n.td,{children:"Indoor unavailable, low accuracy"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"fusion-approaches",children:"Fusion Approaches"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Early Fusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Combine raw sensor data"}),"\n",(0,r.jsx)(n.li,{children:"Joint feature extraction"}),"\n",(0,r.jsx)(n.li,{children:"Example: RGB-D point cloud coloring"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Late Fusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Process sensors independently"}),"\n",(0,r.jsx)(n.li,{children:"Combine at decision level"}),"\n",(0,r.jsx)(n.li,{children:"Example: Voting from multiple detectors"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deep Fusion"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Learn to combine at multiple levels"}),"\n",(0,r.jsx)(n.li,{children:"End-to-end training"}),"\n",(0,r.jsx)(n.li,{children:"Example: Multi-modal transformer networks"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"kalman-filter",children:"Kalman Filter"}),"\n",(0,r.jsx)(n.p,{children:"The fundamental algorithm for sensor fusion:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class KalmanFilter:\n    def __init__(self, dim_x, dim_z):\n        self.x = np.zeros(dim_x)     # State estimate\n        self.P = np.eye(dim_x)       # State covariance\n        self.Q = np.eye(dim_x)       # Process noise\n        self.R = np.eye(dim_z)       # Measurement noise\n        self.F = np.eye(dim_x)       # State transition\n        self.H = np.zeros((dim_z, dim_x))  # Measurement matrix\n\n    def predict(self):\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n\n    def update(self, z):\n        y = z - self.H @ self.x           # Innovation\n        S = self.H @ self.P @ self.H.T + self.R  # Innovation covariance\n        K = self.P @ self.H.T @ np.linalg.inv(S)  # Kalman gain\n        self.x = self.x + K @ y\n        self.P = (np.eye(len(self.x)) - K @ self.H) @ self.P\n"})}),"\n",(0,r.jsx)(n.h3,{id:"extended-kalman-filter-ekf",children:"Extended Kalman Filter (EKF)"}),"\n",(0,r.jsx)(n.p,{children:"For nonlinear systems:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Linearizes around current estimate"}),"\n",(0,r.jsx)(n.li,{children:"Uses Jacobians for state transition and measurement"}),"\n",(0,r.jsx)(n.li,{children:"Standard for robot localization"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"particle-filters",children:"Particle Filters"}),"\n",(0,r.jsx)(n.p,{children:"For highly nonlinear, multi-modal distributions:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Represent belief with weighted samples"}),"\n",(0,r.jsx)(n.li,{children:"Handles non-Gaussian uncertainty"}),"\n",(0,r.jsx)(n.li,{children:"Used in Monte Carlo Localization"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"pose-estimation",children:"Pose Estimation"}),"\n",(0,r.jsx)(n.h3,{id:"6-dof-object-pose",children:"6-DoF Object Pose"}),"\n",(0,r.jsx)(n.p,{children:"Determining object position and orientation:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Approaches:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Keypoint-based"}),": Detect 2D keypoints, solve PnP"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dense correspondence"}),": Predict per-pixel coordinates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Direct regression"}),": End-to-end pose prediction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Render-and-compare"}),": Match against rendered templates"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"human-pose-estimation",children:"Human Pose Estimation"}),"\n",(0,r.jsx)(n.p,{children:"Detecting human body configuration:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2D Pose"}),": Pixel locations of joints"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D Pose"}),": 3D joint positions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Body Mesh"}),": Full body surface reconstruction"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Applications in HRI:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understanding human intent"}),"\n",(0,r.jsx)(n.li,{children:"Safe human-robot interaction"}),"\n",(0,r.jsx)(n.li,{children:"Gesture recognition"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"state-estimation",children:"State Estimation"}),"\n",(0,r.jsx)(n.h3,{id:"robot-state-estimation",children:"Robot State Estimation"}),"\n",(0,r.jsx)(n.p,{children:"Combining multiple sources for robot state:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Robot State Estimation               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                 \u2502\n\u2502  Joint Encoders \u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502                     \u2502                           \u2502\n\u2502  IMU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25ba State \u2500\u2500\u25ba Robot       \u2502\n\u2502                     \u2502    Estimator   State     \u2502\n\u2502  Vision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                           \u2502\n\u2502                     \u2502    (EKF, UKF,             \u2502\n\u2502  Force Sensors \u2500\u2500\u2500\u2500\u2500\u2518     Factor Graph)         \u2502\n\u2502                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h3,{id:"contact-state-estimation",children:"Contact State Estimation"}),"\n",(0,r.jsx)(n.p,{children:"Critical for manipulation and locomotion:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Binary contact"}),": Is there contact?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contact location"}),": Where on the robot?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contact force"}),": How much force?"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"perception-for-manipulation",children:"Perception for Manipulation"}),"\n",(0,r.jsx)(n.h3,{id:"grasp-detection",children:"Grasp Detection"}),"\n",(0,r.jsx)(n.p,{children:"Finding stable grasps for objects:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Approaches:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Analytical methods based on geometry"}),"\n",(0,r.jsx)(n.li,{children:"Data-driven grasp prediction"}),"\n",(0,r.jsx)(n.li,{children:"Reinforcement learning for grasping"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"scene-understanding",children:"Scene Understanding"}),"\n",(0,r.jsx)(n.p,{children:"Building actionable scene representations:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object poses and properties"}),"\n",(0,r.jsx)(n.li,{children:"Support relationships"}),"\n",(0,r.jsx)(n.li,{children:"Affordance detection (what can be done?)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Perception systems are essential for Physical AI:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision Sensors"}),": RGB, depth, and event cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LIDAR"}),": Accurate 3D environmental sensing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Detection"}),": Identifying and localizing objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SLAM"}),": Building maps while localizing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining multiple sensor modalities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State Estimation"}),": Tracking robot and environment state"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Key challenges:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Real-time processing requirements"}),"\n",(0,r.jsx)(n.li,{children:"Handling uncertainty and noise"}),"\n",(0,r.jsx)(n.li,{children:"Adapting to diverse environments"}),"\n",(0,r.jsx)(n.li,{children:"Bridging the sim-to-real gap"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Compare structured light, ToF, and stereo depth sensing methods."}),"\n",(0,r.jsx)(n.li,{children:"Explain the SLAM problem and why it's challenging."}),"\n",(0,r.jsx)(n.li,{children:"What are the advantages of sensor fusion over single sensors?"}),"\n",(0,r.jsx)(n.li,{children:"Describe how the Kalman filter combines prediction and measurement."}),"\n",(0,r.jsx)(n.li,{children:"What is the difference between semantic and instance segmentation?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Exercise 4.1: Perception Pipeline Implementation"})}),"\n",(0,r.jsx)(n.p,{children:"Build a basic perception pipeline that:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Captures RGB-D data from a sensor or simulation"}),"\n",(0,r.jsx)(n.li,{children:"Detects objects using a pre-trained model"}),"\n",(0,r.jsx)(n.li,{children:"Estimates object poses in 3D"}),"\n",(0,r.jsx)(n.li,{children:"Publishes results as ROS 2 messages"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Include visualization in RViz2 showing:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera image with detection overlays"}),"\n",(0,r.jsx)(n.li,{children:"Point cloud with detected objects highlighted"}),"\n",(0,r.jsx)(n.li,{children:"TF frames for detected object poses"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const r={},l=s.createContext(r);function t(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);